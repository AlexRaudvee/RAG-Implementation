{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def upload_document(file_path):\n",
    "    print(\"CHUNKING OF DOCUMENT: ...\")\n",
    "    \n",
    "    # Check file type\n",
    "    if file_path.endswith(\".txt\"):\n",
    "        loader = TextLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        loader = PDFMinerLoader(file_path, concatenate_pages=True)\n",
    "        documents = loader.load()\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unsupported file type. Only .txt and .pdf are supported.\")\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\"], chunk_size=500, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    print(\"CHUNKING COMPLETE!\")\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def initialize_vector_db(docs):\n",
    "    print(\"EMBEDDING THE DOCUMENTS: ...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vector_db = Chroma.from_documents(docs, embeddings, persist_directory=\"./db\")\n",
    "    vector_db.persist()\n",
    "    print(\"EMBEDDING DONE!\")\n",
    "    \n",
    "    return vector_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(vector_db, query, k=3):\n",
    "    return vector_db.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from envvar import GEMINI_API_KEY\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(model_name = \"gemini-1.5-flash-002\", system_instruction=\"When you answer the questions with the provided context, do not say that you rely on context, just answer the question, like pretending that knew this information before it was provided to you.\")\n",
    "\n",
    "\n",
    "def generate_response_with_context(model, context, query):\n",
    "    print(\"PULING KNOWLEDGE TO MODEL: ...\")\n",
    "    full_context = \"\\n\".join([doc.page_content for doc in context])\n",
    "    prompt = f\"Relying on this Context answer my question. But keep in mind that you have to answer this question only relying on this context:\\n{full_context}\\n\\nQuestion:\\n{query}\"\n",
    "    \n",
    "    try:\n",
    "        _response = model.generate_content(prompt)\n",
    "        response = _response.candidates[0].content.parts[0].text\n",
    "        role = _response.candidates[0].content.role\n",
    "        print(\"PULL OF KNOWLEDGE DONE!\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}\")\n",
    "        return f\"Failure: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_workflow(file_path, model, user_question):\n",
    "    # Step 1: Upload Document\n",
    "    docs = upload_document(file_path)\n",
    "    \n",
    "    # Step 2: Initialize Vector Database\n",
    "    vector_db = initialize_vector_db(docs)\n",
    "    \n",
    "    # Step 3: Retrieve Context for User's Question\n",
    "    context = retrieve_context(vector_db, user_question)\n",
    "    [print(\"\\n\\nCONTEXT RETRIVED:\")]\n",
    "    for part in context:\n",
    "        print(part.page_content)\n",
    "        print(\"-------------\")\n",
    "    \n",
    "    # Step 4: Generate Answer with Gemini API\n",
    "    answer = generate_response_with_context(model, context, user_question)\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNKING OF DOCUMENT: ...\n",
      "CHUNKING COMPLETE!\n",
      "EMBEDDING THE DOCUMENTS: ...\n",
      "EMBEDDING DONE!\n",
      "\n",
      "\n",
      "CONTEXT RETRIVED:\n",
      "To get out of Jail you may:\n",
      "-------------\n",
      "To get out of Jail you may:\n",
      "-------------\n",
      "use a \"Get Out Of Jail Free\" card if you have one, or\n",
      "-------------\n",
      "PULING KNOWLEDGE TO MODEL: ...\n",
      "PULL OF KNOWLEDGE DONE!\n",
      "RESPONSE:\n",
      "You can get out of jail by using a \"Get Out Of Jail Free\" card, if you have one.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"documents/monopoly_instructions.pdf\"\n",
    "user_question = \"How to get out of the jail?\"\n",
    "\n",
    "response = main_workflow(file_path, model, user_question)\n",
    "print(\"RESPONSE:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
